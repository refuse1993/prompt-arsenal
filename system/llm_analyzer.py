"""
LLM-based Vulnerability Analyzer
LLMì„ í™œìš©í•œ ì·¨ì•½ì  ë¶„ì„ ë° Remediation ì œì•ˆ
"""

import json
from typing import Dict, List, Optional
from dataclasses import dataclass

try:
    import openai
except ImportError:
    openai = None

try:
    import anthropic
except ImportError:
    anthropic = None


@dataclass
class AnalysisResult:
    """LLM ë¶„ì„ ê²°ê³¼"""
    summary: str
    risk_assessment: str
    priority_vulnerabilities: List[Dict]
    attack_scenarios: List[str]
    remediation_steps: List[str]
    security_recommendations: List[str]


class LLMAnalyzer:
    """LLM ê¸°ë°˜ ì·¨ì•½ì  ë¶„ì„ê¸°"""

    def __init__(self, provider: str, model: str, api_key: str):
        """
        Args:
            provider: 'openai' or 'anthropic'
            model: ëª¨ë¸ ì´ë¦„
            api_key: API í‚¤
        """
        self.provider = provider
        self.model = model
        self.api_key = api_key

    async def analyze_scan_results(self, scan_data: Dict) -> AnalysisResult:
        """
        ìŠ¤ìº” ê²°ê³¼ë¥¼ LLMìœ¼ë¡œ ë¶„ì„

        Args:
            scan_data: ìŠ¤ìº” ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (ports, services, cves)

        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        # LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_analysis_prompt(scan_data)

        # LLM í˜¸ì¶œ
        response = await self._call_llm(prompt)

        # ì‘ë‹µ íŒŒì‹±
        return self._parse_response(response)

    def _build_analysis_prompt(self, scan_data: Dict) -> str:
        """ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        target = scan_data.get('target', 'Unknown')
        open_ports = scan_data.get('open_ports', [])
        services = scan_data.get('services', [])
        findings = scan_data.get('findings', [])

        prompt = f"""ë‹¹ì‹ ì€ ì‚¬ì´ë²„ ë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì‹œìŠ¤í…œ ì·¨ì•½ì  ìŠ¤ìº” ê²°ê³¼ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

## ìŠ¤ìº” ëŒ€ìƒ
- Target: {target}

## ì—´ë¦° í¬íŠ¸
"""
        for port in open_ports:
            prompt += f"- Port {port['port']}/{port['protocol']}: {port['service']} ({port.get('version', 'Unknown version')})\n"

        prompt += "\n## ë°œê²¬ëœ ì·¨ì•½ì \n"
        for finding in findings:
            prompt += f"- [{finding['severity'].upper()}] {finding['title']}\n"
            prompt += f"  {finding['description']}\n"
            if finding.get('cve_id'):
                prompt += f"  CVE: {finding['cve_id']}\n"

        prompt += """
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”:

{
  "summary": "ì „ì²´ ë³´ì•ˆ ìƒíƒœ ìš”ì•½ (2-3ë¬¸ì¥)",
  "risk_assessment": "ìœ„í—˜ë„ í‰ê°€ (Critical/High/Medium/Low) ë° ê·¼ê±°",
  "priority_vulnerabilities": [
    {
      "title": "ì·¨ì•½ì  ì œëª©",
      "severity": "critical/high/medium/low",
      "priority": 1,
      "reason": "ìš°ì„ ìˆœìœ„ ì´ìœ "
    }
  ],
  "attack_scenarios": [
    "ê³µê²© ì‹œë‚˜ë¦¬ì˜¤ 1: êµ¬ì²´ì ì¸ ê³µê²© ë°©ë²• ì„¤ëª…",
    "ê³µê²© ì‹œë‚˜ë¦¬ì˜¤ 2: ..."
  ],
  "remediation_steps": [
    "Step 1: ì¦‰ì‹œ ì¡°ì¹˜í•  ì‚¬í•­",
    "Step 2: ë‹¨ê¸° ì¡°ì¹˜ (1ì£¼ì¼ ì´ë‚´)",
    "Step 3: ì¤‘ì¥ê¸° ì¡°ì¹˜"
  ],
  "security_recommendations": [
    "ë³´ì•ˆ ê¶Œì¥ì‚¬í•­ 1",
    "ë³´ì•ˆ ê¶Œì¥ì‚¬í•­ 2"
  ]
}

**ì¤‘ìš”**: ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""
        return prompt

    async def _call_llm(self, prompt: str) -> str:
        """LLM í˜¸ì¶œ"""
        if self.provider == 'openai':
            return await self._call_openai(prompt)
        elif self.provider == 'anthropic':
            return await self._call_anthropic(prompt)
        else:
            raise ValueError(f"Unknown provider: {self.provider}")

    async def _call_openai(self, prompt: str) -> str:
        """OpenAI API í˜¸ì¶œ"""
        if not openai:
            raise ImportError("openai package not installed")

        client = openai.AsyncOpenAI(api_key=self.api_key)

        # GPT-4/5/o1 ëª¨ë¸ì€ max_completion_tokens ì‚¬ìš©
        token_param = {}
        if self.model.startswith(('gpt-5', 'o1')):
            token_param = {"max_completion_tokens": 2000}
        else:
            token_param = {"max_tokens": 2000}

        response = await client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "You are a cybersecurity expert. Always respond in valid JSON format."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # ì¼ê´€ëœ ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ temperature
            **token_param
        )

        return response.choices[0].message.content

    async def _call_anthropic(self, prompt: str) -> str:
        """Anthropic API í˜¸ì¶œ"""
        if not anthropic:
            raise ImportError("anthropic package not installed")

        client = anthropic.AsyncAnthropic(api_key=self.api_key)

        response = await client.messages.create(
            model=self.model,
            max_tokens=2000,
            temperature=0.3,
            system="You are a cybersecurity expert. Always respond in valid JSON format.",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        return response.content[0].text

    def _parse_response(self, response: str) -> AnalysisResult:
        """LLM ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
            response = response.strip()
            if response.startswith('```json'):
                response = response[7:]
            if response.startswith('```'):
                response = response[3:]
            if response.endswith('```'):
                response = response[:-3]
            response = response.strip()

            data = json.loads(response)

            return AnalysisResult(
                summary=data.get('summary', ''),
                risk_assessment=data.get('risk_assessment', ''),
                priority_vulnerabilities=data.get('priority_vulnerabilities', []),
                attack_scenarios=data.get('attack_scenarios', []),
                remediation_steps=data.get('remediation_steps', []),
                security_recommendations=data.get('security_recommendations', [])
            )

        except json.JSONDecodeError as e:
            print(f"  âš ï¸  LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"  ì‘ë‹µ: {response[:500]}")

            # Fallback: ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
            return AnalysisResult(
                summary="LLM ë¶„ì„ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                risk_assessment="ë¶„ì„ ì‹¤íŒ¨",
                priority_vulnerabilities=[],
                attack_scenarios=[],
                remediation_steps=["ì·¨ì•½ì ì„ ìˆ˜ë™ìœ¼ë¡œ ê²€í† í•˜ì„¸ìš”."],
                security_recommendations=[]
            )

    def format_analysis_report(self, analysis: AnalysisResult) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ì½ê¸° ì‰¬ìš´ ë¦¬í¬íŠ¸ë¡œ í¬ë§·"""
        report = "=" * 80 + "\n"
        report += "ğŸ” LLM ì·¨ì•½ì  ë¶„ì„ ë¦¬í¬íŠ¸\n"
        report += "=" * 80 + "\n\n"

        report += "## ğŸ“Š ìš”ì•½\n"
        report += f"{analysis.summary}\n\n"

        report += "## âš ï¸  ìœ„í—˜ë„ í‰ê°€\n"
        report += f"{analysis.risk_assessment}\n\n"

        if analysis.priority_vulnerabilities:
            report += "## ğŸ¯ ìš°ì„ ìˆœìœ„ ì·¨ì•½ì \n"
            for vuln in analysis.priority_vulnerabilities:
                priority = vuln.get('priority', '?')
                title = vuln.get('title', 'Unknown')
                severity = vuln.get('severity', 'unknown').upper()
                reason = vuln.get('reason', '')
                report += f"{priority}. [{severity}] {title}\n"
                report += f"   ì´ìœ : {reason}\n\n"

        if analysis.attack_scenarios:
            report += "## ğŸ’¥ ê³µê²© ì‹œë‚˜ë¦¬ì˜¤\n"
            for i, scenario in enumerate(analysis.attack_scenarios, 1):
                report += f"{i}. {scenario}\n\n"

        if analysis.remediation_steps:
            report += "## ğŸ”§ ì¡°ì¹˜ ë°©ë²•\n"
            for step in analysis.remediation_steps:
                report += f"- {step}\n"
            report += "\n"

        if analysis.security_recommendations:
            report += "## ğŸ’¡ ë³´ì•ˆ ê¶Œì¥ì‚¬í•­\n"
            for rec in analysis.security_recommendations:
                report += f"- {rec}\n"
            report += "\n"

        report += "=" * 80 + "\n"

        return report

    async def analyze_single_vulnerability(self, cve_data: Dict) -> str:
        """ë‹¨ì¼ CVEì— ëŒ€í•œ ì‹¬í™” ë¶„ì„"""
        prompt = f"""ë‹¤ìŒ CVE ì·¨ì•½ì ì— ëŒ€í•´ ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”:

CVE ID: {cve_data.get('cve_id', 'Unknown')}
Severity: {cve_data.get('severity', 'Unknown')}
CVSS Score: {cve_data.get('cvss_score', 0.0)}
Affected: {cve_data.get('affected_software', 'Unknown')}
Description: {cve_data.get('description', 'No description')}

ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì·¨ì•½ì ì˜ ê¸°ìˆ ì  ì›ë¦¬
2. ì‹¤ì œ ê³µê²© ì‹œë‚˜ë¦¬ì˜¤ (êµ¬ì²´ì ì¸ ì˜ˆì‹œ)
3. ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì™„í™” ë°©ë²•
4. ì¥ê¸°ì ì¸ í•´ê²° ë°©ì•ˆ
"""

        response = await self._call_llm(prompt)
        return response
