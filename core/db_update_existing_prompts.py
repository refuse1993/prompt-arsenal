"""
Update Existing Prompts with Classification
Í∏∞Ï°¥ ÌîÑÎ°¨ÌîÑÌä∏Ïóê Î∂ÑÎ•ò Ï†ïÎ≥¥ Ï∂îÍ∞Ä
"""

import sqlite3
from pathlib import Path


# Í∏∞Ï°¥ Ïπ¥ÌÖåÍ≥†Î¶¨ ‚Üí ÏÉàÎ°úÏö¥ Î∂ÑÎ•ò Îß§Ìïë
CLASSIFICATION_MAP = {
    # Offensive: Jailbreak
    "jailbreak": {
        "purpose": "offensive",
        "risk_category": "security",
        "technique": "jailbreak",
        "modality": "text_only"
    },

    # Offensive: Prompt Injection
    "prompt_injection": {
        "purpose": "offensive",
        "risk_category": "security",
        "technique": "prompt_injection",
        "modality": "text_only"
    },

    # Offensive: Adversarial
    "adversarial": {
        "purpose": "offensive",
        "risk_category": "safety",
        "technique": "adversarial",
        "modality": "text_only"
    },
    "harmful_content": {
        "purpose": "offensive",
        "risk_category": "safety",
        "technique": "adversarial",
        "modality": "text_only"
    },

    # Offensive: Fuzzing
    "fuzzing": {
        "purpose": "offensive",
        "risk_category": "security",
        "technique": "fuzzing",
        "modality": "text_only"
    },

    # Defensive: Security
    "malicious_use": {
        "purpose": "defensive",
        "risk_category": "security",
        "technique": "robustness_test",
        "modality": "text_only"
    },
    "information_hazard": {
        "purpose": "defensive",
        "risk_category": "security",
        "technique": "robustness_test",
        "modality": "text_only"
    },
    "illegal_activity": {
        "purpose": "defensive",
        "risk_category": "security",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },
    "malware_generation": {
        "purpose": "defensive",
        "risk_category": "security",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },
    "fraud": {
        "purpose": "defensive",
        "risk_category": "security",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },
    "privacy_violation": {
        "purpose": "defensive",
        "risk_category": "security",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },

    # Defensive: Safety
    "physical_harm": {
        "purpose": "defensive",
        "risk_category": "safety",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },
    "economic_harm": {
        "purpose": "defensive",
        "risk_category": "safety",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },
    "health_consultation": {
        "purpose": "defensive",
        "risk_category": "safety",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },

    # Defensive: Ethics
    "toxic_content": {
        "purpose": "defensive",
        "risk_category": "ethics",
        "technique": "robustness_test",
        "modality": "text_only"
    },
    "profanity": {
        "purpose": "defensive",
        "risk_category": "ethics",
        "technique": "content_filter_test",
        "modality": "text_only"
    },
    "offensive": {
        "purpose": "defensive",
        "risk_category": "ethics",
        "technique": "content_filter_test",
        "modality": "text_only"
    },
    "hate_speech": {
        "purpose": "defensive",
        "risk_category": "ethics",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },

    # Defensive: Misinformation
    "misinformation": {
        "purpose": "defensive",
        "risk_category": "misinformation",
        "technique": "robustness_test",
        "modality": "text_only"
    },
    "human_impersonation": {
        "purpose": "defensive",
        "risk_category": "misinformation",
        "technique": "robustness_test",
        "modality": "text_only"
    },

    # Defensive: Compliance
    "sexual_content": {
        "purpose": "defensive",
        "risk_category": "compliance",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },
    "political_lobbying": {
        "purpose": "defensive",
        "risk_category": "compliance",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },
    "legal_opinion": {
        "purpose": "defensive",
        "risk_category": "compliance",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },
    "financial_advice": {
        "purpose": "defensive",
        "risk_category": "compliance",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    },
    "gov_decision": {
        "purpose": "defensive",
        "risk_category": "compliance",
        "technique": "safety_benchmark",
        "modality": "multimodal"
    }
}


def update_existing_prompts():
    """Í∏∞Ï°¥ ÌîÑÎ°¨ÌîÑÌä∏Ïóê Î∂ÑÎ•ò Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏"""
    project_root = Path(__file__).parent.parent
    db_path = str(project_root / "arsenal.db")

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    print("üîÑ Updating existing prompts with classification...")

    updated = 0
    skipped = 0

    # Î™®Îì† ÌîÑÎ°¨ÌîÑÌä∏ Í∞ÄÏ†∏Ïò§Í∏∞
    cursor.execute("SELECT id, category FROM prompts")
    prompts = cursor.fetchall()

    for prompt_id, category in prompts:
        if category in CLASSIFICATION_MAP:
            classification = CLASSIFICATION_MAP[category]

            cursor.execute('''
                UPDATE prompts
                SET purpose = ?, risk_category = ?, technique = ?, modality = ?
                WHERE id = ?
            ''', (
                classification['purpose'],
                classification['risk_category'],
                classification['technique'],
                classification['modality'],
                prompt_id
            ))
            updated += 1
        else:
            # Unknown category - Í∏∞Î≥∏Í∞í Ïú†ÏßÄ
            skipped += 1

    conn.commit()
    conn.close()

    print(f"\n‚úÖ Update complete!")
    print(f"  Updated: {updated}")
    print(f"  Skipped: {skipped}")

    # ÌÜµÍ≥Ñ Ï∂úÎ†•
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    print(f"\nüìä Updated Statistics:")

    print(f"\nüéØ Purpose:")
    cursor.execute('SELECT purpose, COUNT(*) FROM prompts GROUP BY purpose')
    for row in cursor.fetchall():
        print(f"  {row[0]}: {row[1]}")

    print(f"\n‚ö†Ô∏è  Risk Category:")
    cursor.execute('SELECT risk_category, COUNT(*) FROM prompts GROUP BY risk_category')
    for row in cursor.fetchall():
        print(f"  {row[0]}: {row[1]}")

    print(f"\nüîß Technique:")
    cursor.execute('SELECT technique, COUNT(*) FROM prompts GROUP BY technique')
    for row in cursor.fetchall():
        print(f"  {row[0]}: {row[1]}")

    print(f"\nüì± Modality:")
    cursor.execute('SELECT modality, COUNT(*) FROM prompts GROUP BY modality')
    for row in cursor.fetchall():
        print(f"  {row[0]}: {row[1]}")

    conn.close()


if __name__ == "__main__":
    update_existing_prompts()
